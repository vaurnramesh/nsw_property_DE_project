version: '3.8'
services:
    postgres:
        image: postgres:13
        networks:
            - airflow
        environment:
        - POSTGRES_USER=airflow
        - POSTGRES_PASSWORD=airflow
        - POSTGRES_DB=airflow
        volumes:
            - postgres-db-volume:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5

    init-airflow:
        build: .
        user: root
        depends_on:
        - postgres
        networks:
            - airflow
        environment:
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
        - GCP_PROJECT_ID=${GCP_PROJECT_ID}
        - GCP_GCS_BUCKET=${GCP_GCS_BUCKET}
        - GOOGLE_APPLICATION_CREDENTIALS=${GOOGLE_APPLICATION_CREDENTIALS}
        - AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=${AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT}
        - PG_HOST=${PG_HOST}
        - PG_USER=${PG_USER}
        - PG_PASSWORD=${PG_PASSWORD}
        - PG_PORT=${PG_PORT}
        - PG_DATABASE=${PG_DATABASE}
        command: >
          bash -c "
          until pg_isready -h postgres -U airflow; do
            echo 'Waiting for PostgreSQL to be ready...';
            sleep 2;
          done;
          airflow db migrate && airflow connections create-default-connections &&
          airflow users create -r Admin -u admin -p admin -e admin@example.com -f admin -l airflow &&
          airflow users create -r Admin -u airflow -p airflow -e airflow@example.com -f airflow -l airflow &&
          echo 'Initialization complete' &&
          touch /opt/airflow/shared/initialized
          "

        healthcheck:
            test: ["CMD-SHELL", "airflow db check && echo 'Database ready!'"]
            interval: 10s
            timeout: 10s
            retries: 5
            start_period: 10s

        volumes:
        - shared-data:/opt/airflow/shared
      
    scheduler:
        build: .
        command: >
            bash -c "
            until [ -f /opt/airflow/shared/initialized ]; do
                echo 'Waiting for init-airflow to complete...';
                sleep 2;
            done;
            airflow scheduler
            "
        depends_on:
            - postgres
            - init-airflow
        networks:
            - airflow
        environment:
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
        - AIRFLOW_UID=$AIRFLOW_UID
        - COMPOSE_PROJECT_NAME=airflow2025_default
        - AIRFLOW__CORE__EXECUTOR=LocalExecutor
        - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10
        - AIRFLOW_CONN_METADATA_DB=postgres+psycopg2://airflow:airflow@postgres:5432/airflow
        - AIRFLOW_VAR__METADATA_DB_SCHEMA=airflow
        - _AIRFLOW_WWW_USER_CREATE=True
        - _AIRFLOW_WWW_USER_USERNAME=airflow
        - _AIRFLOW_WWW_USER_PASSWORD=airflow
        - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
        - AIRFLOW__CORE__LOAD_EXAMPLES=False       
        - GCP_PROJECT_ID=${GCP_PROJECT_ID}
        - GCP_GCS_BUCKET=${GCP_GCS_BUCKET}
        - GOOGLE_APPLICATION_CREDENTIALS=${GOOGLE_APPLICATION_CREDENTIALS}
        - AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=${AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT}
        - PG_HOST=${PG_HOST}
        - PG_USER=${PG_USER}
        - PG_PASSWORD=${PG_PASSWORD}
        - PG_PORT=${PG_PORT}
        - PG_DATABASE=${PG_DATABASE}
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - shared-data:/opt/airflow/shared

    webserver:
        build: .
        command: >
            bash -c "
            until [ -f /opt/airflow/shared/initialized ]; do
                echo 'Waiting for init-airflow to complete...';
                sleep 2;
            done;
            airflow webserver
            "
        depends_on:
            - postgres
            - init-airflow
            - scheduler
        networks:
            - airflow    
        environment:
        - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
        - AIRFLOW_UID=50000
        - COMPOSE_PROJECT_NAME=airflow2025_default
        - AIRFLOW__CORE__EXECUTOR=LocalExecutor
        - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10
        - AIRFLOW_CONN_METADATA_DB=postgres+psycopg2://airflow:airflow@postgres:5432/airflow
        - AIRFLOW_VAR__METADATA_DB_SCHEMA=airflow
        - _AIRFLOW_WWW_USER_CREATE=True
        - _AIRFLOW_WWW_USER_USERNAME=airflow
        - _AIRFLOW_WWW_USER_PASSWORD=airflow
        - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
        - AIRFLOW__CORE__LOAD_EXAMPLES=False
        - GCP_PROJECT_ID=${GCP_PROJECT_ID}
        - GCP_GCS_BUCKET=${GCP_GCS_BUCKET}
        - GOOGLE_APPLICATION_CREDENTIALS=${GOOGLE_APPLICATION_CREDENTIALS}
        - AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=${AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT}
        - PG_HOST=${PG_HOST}
        - PG_USER=${PG_USER}
        - PG_PASSWORD=${PG_PASSWORD}
        - PG_PORT=${PG_PORT}
        - PG_DATABASE=${PG_DATABASE}
        volumes:
            - ./dags:/opt/airflow/dags
            - ./logs:/opt/airflow/logs
            - shared-data:/opt/airflow/shared            

        user: "50000:0"
        ports:
            - "8080:8080"
        healthcheck:
            test: [ "CMD-SHELL", "[ -f /home/airflow/airflow-webserver.pid ]" ]
            interval: 30s
            timeout: 30s
            retries: 3

volumes:
  postgres-db-volume:
  shared-data:

networks:
  airflow:
    external: true
    name: airflow2025_default